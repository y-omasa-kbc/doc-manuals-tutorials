## **演習: PythonによるAzure OpenAI Serviceの利用**

### **1. 演習の目的**

本演習では、Microsoft Azureが提供する「**Azure OpenAI Service**」を、ローカルの**Python**環境から利用する基本的な手法を学習します。

将来的にLangChainなどのフレームワークを用いた高度なAIアプリケーション開発へ進むことを見据え、本演習ではその第一歩として、**AIモデルに対して指示（プロンプト）を送信し、生成結果を受信する**という、最も基本的なAPI連携を実装します。

これにより、クラウドベースの生成AIをプログラムから制御する基礎技術の理解を深めることを目的とします。

### **2. 事前準備**

本演習を開始するにあたり、以下の環境および情報が必要となります。

#### **2.1. Python実行環境 (Windows, VSCode, venv)**

本演習では、Windows PC、Visual Studio Code (VSCode)、およびPythonの標準仮想環境管理ツールであるvenvを使用することを前提とします。

**環境構築手順:**

1. プロジェクトフォルダの作成:  
   任意の場所（例: デスクトップ）に、本演習用のフォルダ（例: azure-openai-practice）を作成します。  
2. VSCodeでフォルダを開く:  
   VSCodeを起動し、「ファイル」メニューから「フォルダーを開く...」を選択し、先ほど作成したフォルダを開きます。  
3. **仮想環境の作成と有効化**:  
   * VSCodeでターミナルを開きます（「表示」>「ターミナル」またはショートカットキー Ctrl+@）。  
   * 開いたターミナルで以下のコマンドを実行し、.venvという名前の仮想環境を作成します。 
    ```
     python -m venv .venv
    ```
   * 続けて、以下のコマンドで作成した仮想環境を有効化（アクティベート）します。  
    ```
     .venv\Scripts\activate
    ```
   * 成功すると、ターミナルのプロンプトの行頭に (.venv) と表示されます。これ以降のコマンドは、この隔離された仮想環境内で実行されます。

#### **2.2. Azure OpenAI Serviceリソースの作成**

Azure OpenAI Serviceを利用するには、まずAzure上にそのサービスの実体となる「リソース」を作成する必要があります。

**リソース作成手順:**

1. Azure Portalにサインイン:  
   Webブラウザで Azure Portal にアクセスし、ご自身のアカウントでサインインします。  
2. リソースの作成を開始:  
   ポータルの上部にある検索バーで「Azure OpenAI」と入力し、表示されたサービス一覧から「Azure OpenAI」を選択します。次に、表示された画面で「作成」ボタンをクリックします。  
3. 基本情報の入力:  
   「Azure OpenAI を作成する」の画面で、以下の項目を順番に入力・選択します。  
   * **サブスクリプション**: 利用するサブスクリプションを選択します。（通常は一つだけ表示されます）  
   * **リソース グループ**: 「**新規作成**」をクリックし、任意のリソースグループ名（例: openai-rg）を入力して「OK」をクリックします。リソースグループは、関連するリソースをまとめるためのフォルダのようなものです。  
   * **リージョン**: サービスを展開する地域を選択します。特別な理由がなければ、日本国内の「**Japan East (東日本)**」を選択することを推奨します。  
   * **名前**: 作成するリソースの一意の名前を入力します。この名前はAzure全体でユニークである必要があります。（例: 自分の名前-aoai-2025 など）。入力した名前の下に緑のチェックマークが表示されれば利用可能です。  
   * **価格レベル**: 「**Standard S0**」を選択します。  
4. ネットワークとタグの設定:  
   「次へ: ネットワーク >」をクリックします。ネットワーク設定は「すべてのネットワーク (パブリック インターネットからのアクセスを含む) でこのリソースにアクセスできます」が選択されていることを確認し、「次へ: タグ >」をクリックします。タグは省略可能なので、そのまま「次へ: 確認および作成 >」をクリックします。  
5. 作成の実行:  
   入力内容の最終確認画面が表示されます。「検証に成功しました」と表示されたことを確認し、「作成」ボタンをクリックします。デプロイ（リソースの配置）が開始され、完了するまで数分かかります。完了すると「デプロイが完了しました」と表示されます。

#### **2.3. Azure OpenAI Serviceに関する資格情報**

リソース作成後、プログラムから接続するための資格情報を取得します。

**資格情報の取得手順:**

1. 作成したリソースへ移動:  
   デプロイ完了画面で「リソースに移動」ボタンをクリックするか、Azure Portalの検索バーから先ほど作成したリソース名を検索して開きます。  
2. **キーとエンドポイントの取得**:  
   * リソースメニューの左側にある「**キーとエンドポイント**」セクションを選択します。  
   * 画面に「キー1」「キー2」および「エンドポイント」が表示されます。  
   * 「**キー1**」の値をコピーして、プログラムの api_key として使用します（キー2でも可）。  
   * 「**エンドポイント**」のURLをコピーして、プログラムの azure_endpoint として使用します。  
3. **モデルのデプロイとデプロイ名の取得**:  
   * 作成したリソースの「概要」ページを開きます。  
   * ページの中央付近にある「**Azure AI Foundry Portalに移動**」（または旧名称の「Azure OpenAI Studioに移動」）というボタンを探してクリックします。  
   * Azure AI Foundry Portalが開いたら、左側のナビゲーションメニューから「**デプロイ**」（Deployments）を選択します。  
   * **【重要】一覧が空の場合は、以下の手順でモデルをデプロイします。**  
     1. 「**+ モデルのデプロイ**」ボタンをクリックします。  
     2. 表示されるメニューから「**基本モデルをデプロイする**」を選択します。  
     3. **モデルを選択**: gpt-4o-mini を選択します。（利用可能なモデルが表示されます）  
     4. **デプロイ名**: Pythonコードで使用する名前を半角英数で入力します。混乱を避けるため、モデル名と同じ gpt-4o-mini と入力することを推奨します。  
     5. 「**作成**」ボタンをクリックします。デプロイが完了するまで少し待ちます。  
   * デプロイが完了すると、一覧に項目が表示されます。その「**デプロイ名**」（例: gpt-4o-mini）をコピーしてください。これがプログラムの model に設定する値となります。

#### **2.4. openaiライブラリ**

* Azure OpenAI Serviceと通信するためのPythonライブラリです。  
* **2.1で有効化した仮想環境内で**、ターミナルから以下のコマンドを実行してインストールしてください。  
  ```
  pip install openai
  ```
### **3. 実装と実行手順**

#### **ステップ1: Pythonファイルの作成**

VSCodeのエクスプローラービューで、フォルダ内にazure_openai_exercise.py等のファイル名で新しいPythonファイルを作成します。

#### **ステップ2: コードの記述**

作成したファイルを開き、以下のサンプルコードを記述します。コード内のプレースホルダー (YOUR_...) は、**2.3**で取得した各自の資格情報に置き換えてください。
```python
# 必要なライブラリをインポート  
import os  
from openai import AzureOpenAI

# Azure OpenAI Serviceへの接続クライアントを初期化  
# 事前に準備した自身の資格情報を設定  
client = AzureOpenAI(  
    azure_endpoint = "YOUR_AZURE_OPENAI_ENDPOINT", # エンドポイント  
    api_key = "YOUR_AZURE_OPENAI_KEY",             # APIキー  
    api_version = "2024-02-01"                     # APIバージョン  
)

# AIモデルへの指示（プロンプト）を定義  
prompt_message = "クラウドコンピューティングとAIの関係性について、技術的な観点から解説してください。"

print(f"プロンプト: {prompt_message}")  
print("----------------------------------------")  
print("モデルからの応答:")

try:  
    # Chat Completions APIを呼び出し、モデルからの応答を生成  
    response = client.chat.completions.create(  
        model = "YOUR_AZURE_OPENAI_DEPLOYMENT_NAME",  # デプロイ名  
        messages = [  
            # システムメッセージでAIの役割や振る舞いを定義  
            {"role": "system", "content": "You are an AI assistant that provides detailed technical explanations."},  
            # ユーザーメッセージで具体的な指示を送信  
            {"role": "user", "content": prompt_message}  
        ],  
        max_tokens=400,  # 生成応答の最大トークン数  
        temperature=0.7, # 応答の多様性を制御（0に近いほど決定論的になる）  
    )

    # 応答オブジェクトからコンテンツ部分を抽出して表示  
    print(response.choices[0].message.content)

except Exception as e:  
    print(f"エラーが発生しました: {e}")
```
##### **サンプルコードの解説**

* from openai import AzureOpenAI: openaiライブラリから、Azure OpenAI Serviceと通信するためのクラスをインポートします。  
* client = AzureOpenAI(...): 資格情報（エンドポイント、APIキー）を使って、サービスへ接続するための「クライアントオブジェクト」を作成します。このclientを介してAIとのやり取りを行います。  
* response = client.chat.completions.create(...): ここがAIに応答生成を依頼する中心部分です。  
  * model: どのデプロイメント（AIモデル）を利用するかを指定します。  
  * messages: AIとの対話履歴をリスト形式で渡します。roleにはsystem（AIの役割設定）、user（利用者からの指示）、assistant（AIからの過去の返答）のいずれかを指定します。  
  * print(response.choices[0].message.content): AIからの応答は複雑なオブジェクト形式で返ってくるため、その中から人間が読めるテキスト部分を取り出して表示しています。

##### **パラメータの詳細解説: max_tokensとtemperature**

createメソッドに渡すパラメータを調整することで、AIの応答を細かく制御できます。中でも特に重要なのがmax_tokensとtemperatureです。

* max_tokens (最大トークン数)  
  max_tokensは、AIが生成する応答の最大長を制限するためのパラメータです。「トークン」とは、AIがテキストを処理する際の単位で、おおよそ単語や句読点に対応しますが、完全に一致するわけではありません。例えば、英語の"hello"は1トークンですが、日本語の「こんにちは」は複数のトークンとして扱われます。  
  * **値を大きくする**: より長い文章を生成できますが、その分、応答生成にかかる時間とコストが増加します。  
  * 値を小さくする: 応答が短くなり、途中で文章が切れてしまう可能性があります。要約や短い回答が欲しい場合に有効です。  
    注意点: AIモデルには、一度に処理できるトークン数の上限（プロンプトと生成応答の合計）が定められています。これを「コンテキストウィンドウ」と呼びます。max_tokensを非常に大きな値に設定しても、この上限を超えることはできません。  
* temperature (温度)  
  temperatureは、応答の「ランダム性」や「創造性」を制御するための非常に重要なパラメータです。値は通常0から2の間で設定します。  
  * **temperatureを低くする (例: 0.1〜0.3)**  
    * AIは、最も確率が高いと予測される単語を選びやすくなり、**応答がより決定的で一貫性のあるもの**になります。  
    * **用途**: 事実に基づいた質問への回答、文章の分類、決まった形式でのテキスト生成など、正確さや予測可能性が求められる場合に適しています。  
    * **例**: 「日本の首都はどこですか？」という質問に対しては、毎回「東京」と安定して答えてほしいので、低いtemperatureが望ましいです。  
  * **temperatureを高くする (例: 0.7〜1.5)**  
    * AIは、確率が低い単語も選択肢に含めるようになり、**応答がより多様で、独創的、あるいは予期しないもの**になります。  
    * **用途**: アイデアのブレインストーミング、詩や物語の創作、多様な選択肢の提案など、創造性が求められる場合に適しています。  
    * 例: 「新しいSF小説のあらすじを考えて」といった指示には、高いtemperatureを設定することで、毎回異なるユニークなアイデアを得やすくなります。  
      この演習で設定されている0.7は、ある程度の正確性を保ちつつ、不自然にならない範囲で多様な表現を許容する、バランスの取れた一般的な値です。

#### **ステップ3: プログラムの実行**

**仮想環境が有効化された状態**のVSCodeターミナルで、以下のコマンドを実行します。
```
python azure_openai_exercise.py
```
#### **ステップ4: 実行結果の確認**

プログラムを実行すると、ターミナル上にAIモデルからの応答が出力されます。

### **4. 補足: モデルの選択とコストについて**

Azure OpenAI Serviceでは複数のAIモデルを利用でき、**どのモデルを選択するかによって、性能とコストが大きく異なります。**

#### **モデルによる性能の違い**

一般的に、モデルは以下のような性能の序列になっています。  
高性能 gpt-4o > gpt-4o-mini > gpt-3.5-turbo 低性能

* **gpt-4o**: 現時点で最も高性能なモデルの一つ。複雑な推論、難解な指示の理解、創造性の高いタスクに優れています。  
* **gpt-4o-mini**: 本演習で利用するモデル。最新世代の小型モデルで、旧世代のgpt-3.5-turboよりも高性能でありながら、コストを抑えて利用できます。多くのタスクにおいて十分な性能を発揮します。  
* **gpt-3.5-turbo**: 比較的単純なタスクを高速に処理することを得意とします。

#### **モデルによるコストの違い**

利用料金は、プログラムがAIに送信したテキスト（入力）と、AIが生成したテキスト（出力）の量、すなわち**「トークン数」に応じた従量課金制**です。そして、その**トークンあたりの単価がモデルによって異なります。**

**高性能なモデルほど、トークン単価は高く設定されています。**

| モデル | 性能 | コスト | 特徴 |
| :---- | :---- | :---- | :---- |
| **gpt-4o** | 高い | 高い | 最高レベルの性能を求める場合に選択。 |
| **gpt-4o-mini** | 中〜高 | 中 | **本演習で推奨。**性能とコストのバランスに優れる。 |
| **gpt-3.5-turbo** | 標準 | 低い | とにかくコストを抑えたい、または単純なタスクの場合に選択。 |

学習目的での利用:  
本演習のように学習目的で利用する場合、まずはgpt-4o-miniのようなコストと性能のバランスが取れたモデルから始めることを強く推奨します。  
注意:  
料金体系は更新される可能性があるため、最新かつ正確な情報は、必ずAzure OpenAI Serviceの公式料金ページで確認してください。

### **5. 応用的課題**

AIモデルの応答は、プロンプトの設計に大きく依存します。  
prompt_message変数や、APIリクエストのパラメータを様々に変更し、応答がどのように変化するかを検証してください。

* **プロンプトの多様化**:  
  * **事実に基づく質問**: "日本の行政機関における三権分立について説明せよ。"  
  * **アイデア生成**: "再生可能エネルギーの普及を促進するための新たな政策アイデアを3点提案せよ。"  
  * **文章生成**: "「デジタルデバイド」をテーマとする短い論考を作成せよ。"  
  * **コード生成**: "Pythonを用いて、フィボナッチ数列を生成する関数を記述せよ。"  
* **パラメータの調整**:  
  * max_tokens: 生成されるテキストの長さを制限するパラメータ。値を変更し、応答の長さを制御してください。  
  * temperature: 値が低いほど決定論的で一貫した応答に、高いほど多様で創造的な応答になります。この値を0.1や1.0などに変更し、応答の傾向の違いを観察してください。

本演習で習得した技術は、対話型AIシステム、文章要約・生成ツール、コード自動生成など、多岐にわたるアプリケーション開発の基礎となります。